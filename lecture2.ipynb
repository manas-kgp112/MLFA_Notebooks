{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello there, Welcome back :)\n",
    "\n",
    "Today we will be looking into locally weighted regression, classification and K-NN learning model. So let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Locally Weighted Regression : The basic idea behind locally weighted regression is to :\n",
    "1) Consider only the \"k\" closest training samples to test sample.\n",
    "2) Use these training sample to fit in a linear function [y=f(x)]\n",
    "3) Proceed same as linear regression and your model is good to go.\n",
    "\n",
    "![#](images/locally-weighted-regression-plot.png \"Locally Weighted Regression Plot\")\n",
    "\n",
    "Now the question comes, how to select k closest training samples beacuse increasing \"k\" may lead to consideration of those training samples which are misleading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLASSIFICATION : It is another type of supervised learning which gives discrete set (non-continous) of output labels. for e.g. loan approval application can output either 'yes' or 'no' as output.\n",
    "\n",
    "What is the idea behind classification algorithm : The image prefectly answers the question.\n",
    "\n",
    "![#](images/classification-pictorial.png \"Classification Idea\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-NN/K-Nearest Neighbour : This algorithm is a supervised learning method used in both classification and regression. It works on the principle that \"If you want to know someone’s character, look at its neighbor\" i.e. if it walks like a dog, barks like a dog, then it’s likely to be a dog..\n",
    "\n",
    "**In classification, using K-NN, \"output label of test sample is the majority of output labels in k nearest training samples\".\n",
    "\n",
    "Some definitions before we look into Mathematics and working of Nearest Neighbour Algorithm.\n",
    "\n",
    "1) Nearest Neighbuor : Sample which is at least distance from the test sample.\n",
    "\n",
    "![#](images/nearest-neighbour.png \"Neaest Neighbour Pictorial\")\n",
    "\n",
    "Image shows 1 NN, 2NN, 3NN respectively.\n",
    "\n",
    "Considering an example to understand K-NN in a better way.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K : Selection Issue\n",
    "\n",
    "We must be very alert while choosing the value of k as a 'very low' value of k may make model 'sensitive to noise points' and a 'very high' value of k may include 'misleading points from different classes'. As a general practice, k is chosen to be sqrt(N) where N is the number of training samples. But it may vary from case to case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Issues with Nearest Neighbour Algoritm : \n",
    "\n",
    "1) Expensive - Distance calculation of all training samples and test sample takes too much time and increses computational cost.\n",
    "\n",
    "2) Storage - It forces machine to store all training sample and their distances computed.\n",
    "\n",
    "3) Dimesionality - Increased dimensionality increases training data and also the computational cost.\n",
    "\n",
    "One effecient way to resolve these issues is 'Condensed K-Nearest Neighbour'. We will have a look over it in next few notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Too many terms in one notebook to remember :) Let's take a break !\n",
    "\n",
    "In next notebook we will learn about Condensed K-Nearest Neighbour Algoritm, Logistic Regression CLassifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "da744e63d7114826029aa1cdcb8ffc532c4563260d851055c1d03876e527cfe3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
